---
layout: post
---

先纠错吧, 过去两篇博文里有些不够正确的的东西.

## 一. 遍历文件

古代人类都用 `find(1)`, 我也进行了足够多的讨论了, 这里要说的是利用 bash4 引入的新特性, `** globbing`.

### 1.1 模式匹配

就是找文件啦.

```
shopt -s globstar
for f in **; do echo "$f"; done
```

可以发现居然连名字里包含空格和特殊字符的文件名都能处理好, 大块人心, 再也不必使用 `find -0 | xargs -0` 之类的丧心病狂的不被坑到删光文件都算好的潜规则了.

以此来做过滤的话会少很多心智负担, 比如不用操心 ERE 语法, 也不必记住 `find(1)` 的繁多的选项.

| find(1) | ** |
|:--------|:-------|
|`find . -name *.jpg`| `for f in **/*.jpg; do echo "$f"; done`|
|`find . -type d`|`for f in **; do [ -d "$f" ] && echo "$f"; done`|
|`find . -path '*/systrace/*.py'`|`for f in **/systrace/**/*.py; do echo "$f"; done`|
|`find -iname foler.jpg`|`for f in **; do [[ "${f,,}" == folder.jpg ]] && echo "$f"; done`|
|`find -regex '.*folder\.\(jpg\|png\)' -regextype posix-extended`| `for f in **; do grep -Pq 'folder\.(jpg|png)' <<<"$f" && echo "$f"; done`|
|`find -maxdepth 2`|`for f in */* *; do echo "$f"; done`|
|`find -path '*/.git/*' -prune -o -print`|`for f in **; do [[ "$f" == */.git/* ]] || echo "$f"; done`|

还处理啥的 `$'\0'` 啊....

但是要承认, `find(1)` 的剪枝肯定比遍历过滤更快, 而且有些过滤我采用正则匹配肯定不全对, 你根本想象不到文件名里都有啥.

方便对 bash 不熟悉的同学查阅, 上面出现的语法有: Parameter Substitution, PCRE, Here String, List Constructs, Extended Test Command.

### 1.2 处理文件名

古代人类会用 `find -exec cmd {} \;` 这种奇怪的语法, 甚至想出了 `find -exec bash -c 'echo $0' {} \;` 这种通用表达;

或者使用 `find -0 | xargs -0 -I{} bash -c 'echo $0' {}`, 为了能够使用一点多核的性能沾沾自喜;

但是在 `**` 面前都是小丑, 自然而然地写循环, 还可以处理好 side effects, 有啥不好的吗.

```
shopt -s globstar
for f in **; do filter "$f" && process "$f" && ((cnt++)); done
```

不过要承认, `xargs(1)` 的进程池在裸 bash 下并不容易实现, 如果真有多核需求(我就有过!), 请用 `xargs(1)`

## 二. 多行处理

在过去, 我分享使用 awk / vim / pcregrep 来处理多行, 这次我想说的是, 你可能最多需要 awk + grep, 其他的情况写 Python.

### 2.1 找整株进程树

我用 pcregrep 写过一个正则可以很漂亮地抠出来:

```
$ ps -ef --forest | pcregrep -M '^\w+\s+'$PID'.*\n(.*?\\_.*\n)*'
```

然而实际上我一次都没用到过, 真实场景下, 我每次要查看进程树都是 `ps -ef --forest | grep $PID -C10` 简单粗暴, 但是非常够用.

好了你说抠出来的多行数据不精确, 来个需要精确的场景.

### 2.2 从 JSON 数据里抠 object

```
{
  "adsfraud_main_test_sg": [
    {
      "addr": "10.65.136.28:31866",
      "host": "10.65.136.28",
      "port": 31866,
      "re_registered_times": [],
      "registered_time": 1542740589
    }
  ],
  "amp_render_test_id": [
    {
      "addr": "10.65.136.135:31008",
      "host": "10.65.136.135",
      "lb_port": null,
      "port": 31008,
      "re_registered_times": [],
      "registered_time": 1542536853
    }
  ]
}
```

要把指定 key(`amp_render_test_id`) 的 value(后面的 list) 抠出来, 难点是闭合方括号, 其中还可能有嵌套.

我上次分享的方案是利用 `vim -es` 运行 `normal` ex 命令, 核心是 normal mode 想的 `%` 能够跳转闭合括号, 写出来特别花哨, 我在五分钟内根本就写不出来.

所以现在我强烈建议, 不写通用代码, 写特定场景的实现, 比如这里的两个简单实现就是:

```
awk '/amp_render_test_id/,/^\s+]/'
```

和

```
awk '/amp_render_test_id/,/}/'
```

第一种做法是匹配 `^  ]` 那一行, 第二种是匹配 `^    }`; 你当然会说, 这只是特定数据才能这样, 你能保证不会嵌套 object 吗, 之类的, 对, 我就是针对这特定数据写的匹配, 我可以保证里面只有没有嵌套对象, 所以解决方案简单, 快速, 实用.

这里我要强调一下, 对于一次性的, 功能性的, bash 脚本, 请一定以 **了解自己的数据** 作为基本准则, 了解自己的数据能让任务简单, 世界美好.

### 2.3 段落

```
job1:
    cmd: job1
    schedule: "0 18 * * *"
    kind: cronjob

job2:
    kind: cronjob
    cmd: job2
    schedule: "50 18 * * *"

job3:
    kind: daemon
    cmd: ./manage.py corpus_patch_es
```

找到所有 `kind` 是 `cronjob` 的任务的 `cmd`.

之前我说, 用 `vim -es`, 不, 我五分钟内根本写不出来, 朴素一点, awk 处理段落正好.

```
awk '/cronjob/' RS='' | awk '/cmd/ {print $2}'
```

第一个 awk 的重点是 `RS=''`, 代表 `Row Separator` 是 `\n\n`; 第二个 awk 就是正常的 line stream, 匹配 `cmd` 然后打印第二字段.

不过, 要我说, 这种精确的做法都复杂了, 如果你足够了解你的数据, `grep cronjob -A3 | grep cmd` 都足够了, 意思是找到 `cronjob` 的行并打印之后三行, 然后二次过滤出 `cmd` 行.

**了解你的数据.**

## 三. 任务编排

新知识了, 快醒醒.

大家都熟悉管道, 简直像匕首一样好用, 但是它两个问题.

### 3.1 子进程

简单点, 统计一个文本文件里一个单词出现的频数.

```
KEY=the
cat file.txt | while read line; do for word in $line; do [[ "$word" == "$KEY" ]] && ((cnt++)); done done
echo $cnt
```

会发现 echo 出来的是空行.

问题出在 pipeline 里的每个进程都是 bash 的子进程, 所以:

1. 父进程的 `$KEY` 不能被继承到子进程里, 除非是环境变量
2. 子进程的 `$cnt` 不能更新到父进程的变量空间.

我们有三种做法来改进:

#### 1. 让 `$KEY` 作为环境变量, 然后 `$cnt` 作为标准输出.

总之就是常规的 bash IPC 方案: 环境变量, stdio.

```
export KEY=the
cnt=$(cat file.txt | (while read line; do for word in $line; do [[ "$word" == "$KEY" ]] && ((cnt++)); done done && echo $cnt))
echo $cnt
```

可以看到管道第二段变成了一个 Subshell, 追加了一条 `echo $cnt`, 然后父进程(主 bash) 通过标准输出捕获.

#### 2. 设置 `lastpipe`

bash4.2 新加入了 `lastpipe` option, 可以让管道的最后一个命令不运行在 Subshell 里, 从而可以读取普通变量, 产生副作用.

```
shopt -s lastpipe
KEY=the
cat file.txt | while read line; do for word in $line; do [[ "$word" == "$KEY" ]] && ((cnt++)); done done
echo $cnt
```

不过要主义 `lastpipe` 要启用的另一个必要条件是 `job control not active`:

```
lastpipe
    If set, and job control is not active, the shell runs the last command of a pipeline not executed in the background in the current shell environment.
```

所以如果是在 interactive bash 里运行, 别忘了 `set +m` 关闭 job control; 或者运行 bash 脚本就没问题.

#### 3. Process Substitution

proc sub 的力量是解构式的, 颠覆性地破坏 pipeline 的线性数据流, 直接看代码:

```
KEY=the
while read line; do for word in $line; do [[ "$word" == "$KEY" ]] && ((cnt++)); done done < <(cat file.txt)
echo $cnt
```

完全避免了 pipeline 和子进程, 一个进程内变量飞来飞去其乐融融.

那你会说, 如果我管道里有多处需要处理副作用怎么办?

#### 4. exec + mkfifo

比如要这种 pipeline 是这样的:

```
ls -la | while read line; do ((cnt1++)) && echo $line; done | while read line; do for word in $line; do ((cnt2++)); done done
```

第二段命令统计行数, 第三段统计词数, 如果用一个 proc sub 是不行的, 你会写成这样:

```
while read line; do ((cnt1++)) && echo $line; done < <(ls -la) > >(while read line; do for word in $line; do ((cnt2++)); done)
```

那么统计词数的命令依然会在一个 proc sub 的子进程里, 副作用没了.

这种时候 fifo 作为管道替代品一下就很有价值了:

```
fifo=$(mktemp -u)
mkfifo $fifo
exec 3<>$fifo
rm -f $fifo

while read line; do ((cnt1++)); echo $line; done < <(ls -la) >&3
while read -t1 line; do for _ in $line; do ((cnt2++)); done done <&3
exec 3>&-
echo lines: $cnt1 words: $cnt2
```

第一段创建在临时目录下创建 fifo, 然后 unlink fifo 文件, 但是由于已经 exec 打开了文件描述符, 所以进程里的 fifo 不受影响, 这是为了 cleanup.

然后接下来就是正常的两句有副作用的命令, 只是重定向一下, 很好理解.

把 exec + fifo 做成一个函数, 之后我们还会用到:

```
function execfifo() {
    fifo=$(mktemp -u)
    mkfifo $fifo
    exec $1<>$fifo
    rm -f $fifo
}
```

那么正式进入编排.

### 3.2 编排

pipeline 最大的局限性是它只支持线性, 流式, 行缓冲的多命令通过 stdio 进行 IPC.

![pipe](https://github.com/jschwinger23/jschwinger23.github.io/blob/master/data/my-bash-report/pipeline.png?raw=true)

而假如我想要的是这样的:

![task](https://github.com/jschwinger23/jschwinger23.github.io/blob/master/data/my-bash-report/complex-task.png?raw=true)

cmd3 通过 cmd2 的 stdout 来判断自己的运行模式, 同时 cmd3 也接受 cmd1 的流式输出.

我把这需求叫做 bash 任务编排.

来个具体的例子, 如果一个文本文件里的 `the` 出现次数大于 10, 我就打印出出现过 `the` 的所有行.

为什么用 pipeline 很难实现, 我们来尝试一下:

```
function count() { while read line; do for word in $line; do [[ "$word" == "$1" ]] && ((cnt++)); done done; echo $cnt; }
cat file.txt | tee >(count the) | grep the
```

好的, 我们已经利用 `tee` 来保证第一个命令的输出能够分发两遍, 用 proc sub 来统计 `the` 出现的次数是否大于 10, 用 grep 过滤, 但是最大的问题是 `grep the` 这个进程如何与 `count the` 进程 IPC? `grep` 进程如果获得 `count` 进程的结果?

利用上一话中的 exec + mkfifo 大法我们可以比较容易地实现这一套:

```
execfifo 3
cat file.txt | tee >(count the >&3) | (read -u3 cnt; [[ "$cnt" -gt 10 ]] && grep the)
```

我们可以看到所有的 fd 3 是 IPC 的重要纽带:

![explain](https://github.com/jschwinger23/jschwinger23.github.io/blob/master/data/my-bash-report/explain.png?raw=true)

用这种方式我们几乎可以完成任意的任务编排, 同时就算有副作用的要求也不用畏惧:

```
function read_t() { while read -t0.1 line; do echo $line; done }

execfifo 3; execfifo 4
cat file.txt | tee >(cat >&3) >&4
count the <&3
[[ "$cnt" -gt 4 ]] && grep the < <(read_t <&4)
```

上面这种实现, 所有的命令都没有创建子进程, 所有的变量都在当前 bash 里产生副作用, 坏处是我们要手动维护 fd 代替管道 stdio.

有了这种思路, 任意的 pipeline 都可以转换成副作用模式:

![pipeline](https://github.com/jschwinger23/jschwinger23.github.io/blob/master/data/my-bash-report/pipeline-principle.png?raw=true)

```
cmd1 | cmd2 | cmd3
```

变成

![execfifo](https://github.com/jschwinger23/jschwinger23.github.io/blob/master/data/my-bash-report/execfifo-principle.png?raw=true)

```
execfifo 3; execfifo 4
cmd1 >&3
cmd2 < <(read_t <&3) >&4
cmd3 <(read_t <&4)
```

对了, 那个 `read_t` 是因为没有 stdio 的自动 EOF, 我们只能手动设置 read timeout 0.1s, 不过不用担心 0.1s 不够用, 运行到每一句命令的时候, 所有内容已经在 fd 的 buffer 里了, 所以如果产生了 0.1s 的超时, 那一定是读完了.

由此我们脱离了 pipeline 的约束, 任意的 DAG, 副作用, 我们都能上下其手.

## 四. 我从来没提到过的语法

当然我没提到过的 bash 语法很多, 比如 Brace Extension; 这里我主要聊聊我无视了的大型语法.

### 4.1 array

array 在 bash 里的应用, 其实我见得最多的还是类似 `random.choice(seq)` 这种需求, 从几个 candidates 里随机取值.

那我说两个大家可能不知道的 array 的神奇特征.

#### 1. value as array

bash 保证了任意的 array 操作在一个简单的值对象上是可运行的:

```
a=1
echo ${a[*]}
a+=(2)
for i in ${!a[*]}; do echo ${a[$i]}; done
```

你会发现根本没有必要声明变量类型: `declare -a a`, 而 append 操作居然就是 `+=`, 十分简单.

#### 2. Parameter Substitution

对 array 进行 para sub, 居然是 map 语义!

比放说对 array 变量 `$a` 执行 `${a/old/new}`, 居然是把 array 里的每个值进行一次 `s/old/new` 的替换;

同理 `${a%,*,*}` 也是对每个值进行 `string.rsplit(',', 3)[0]` 操作:

```
a=(o,n,e t,w,o t,h,r,e,e)
for v in ${a[*]%,*,*}; do echo $v; done
#o
#t
#t,h,r
```

很神奇对吧!

不过遗憾的是, 尽管我熟练掌握它的各种特性, 却一次都没遇到过真实的场景要用... 我觉得原因主要是真实世界里, 过滤往往是刚需, 也就是 `filter | map` 组合, 而 array 只提供了 map 还是不够实用.

### 4.2 associative array

就是 Go 的 map, Python 的 dict 啦.

其实还是好用的, 比方说来做经典的统计词频的题 (leetcode 192), awk 是这样写的:

```
awk '{for(i=1; i<=NF; i++) m[$i]++} END {for(w in m) print w, m[w] }' words.txt | sort -k2nr
```

如果用 bash4 提供的 associative array 的话:

```
declare -A m
while read l; do for w in $l; do m[$w]=$((m[$w]+1)); done done <words.txt
for i in ${!m[*]}; do echo $i, ${m[$i]}; done | sort -k2nr
```

也没问题, 我都是一次性 bugfree 写出来的.

不过, 我依然从来没在真实战场上用过...

### 4.3 coproc

这玩意儿是用来做我上文说到的任务编排的, 基本就是后台跑一个进程, 提供一些变量和它 stdio 交互, 比如之前的那个编排问题可以写成:

```
coproc { count the; }
cat file.txt | tee ${COPROC[1]} | (read -u ${COPROC[0]}) cnt; [[ "$cnt" -gt 10 ]] && grep the)
```

很好理解, `${COPROC[0]}` 和 `${COPROC[1]}` 分别是 coproc 进程的 `/dev/stdout` 和 `/dev/stdin`.

不, 我骗你的, 上面那种美好的代码, 怎么可能在 bash 里顺利运行.

四个坑:

1. coproc 里的进程 stdout 不再是 terminal, 所以多数进程采用块缓冲, 如果你放一个 `grep` 进去会发现读不到输出, 因为被块缓冲了
2. coproc 里的进程运行结束就退出了, 然后关闭它的 stdout, 而很多情况你还没来得及读, fd 就被关了..
3. coproc 里的进程读不到 EOF 所以永远不退出, 导致下游读 `${COPROC[0]}` 的进程永远不退出
4. 只能有一个 coproc

由于实在太坑了, 我压根不想调试这段代码, 总之最终形态大约是这样:

```
coproc { grep -q --line-buffered ModRevision; echo $?; }
3<&${COPROC[0]} tee  >(cat >&${COPROC[1]}) < <(etcdctl get /b -w fields) > >(read -u 3 found; [ "$found" = 0 ] && cat)
```

我们还是好好用 execfifo 吧...

## 五. 对工程的思考

### 5.1 我在什么时候使用 bash

由于我已经写了三篇博文关于 bash 了, 导致有些人以为我是什么 `bash 布道者`, 不, 我不是; 相比 `bash 布道者`, 我宁愿自己被称作 `bash 原教旨主义`, 就是旗帜鲜明地反对使用 zsh 等奇怪的无法兼容 bash 的 shell.

从历史里找了几个比较长的 bash 命令, 看看我日常工作的使用情况:

#### 1. diff

```
vimdiff <(for f in **; do [[ "$f" == */systrace/*.py ]] && echo "$f"; done | sort -u) <(for f in **/systrace/**/*.py; do echo "$f"; done | sort -u)
```

这句命令是我写这篇博客时, 为了验证 `**/path/**/*.suffix` 的行为是符合预期的, 对两句命令的结果做了一个 vimdiff, 使用了 proc sub.

#### 2. batch operation

```
cli container remove -f $(cli container list 2>&1 | ggrep -Po '\w{64}' | xargs)
```

这句是批量删除容器, 使用了 `gnu/grep` 抠出容器 ID, 然后 xargs 把结果打平, cmd sub 扔给外层命令.

#### 3. inspect env

```
for e in $(cat /proc/$PID/environ | xargs -0); do echo $e; done | grep -i calico
```

这是我查看一个进程的环境变量, 由于 `/proc/$PID/environ` 的变量是 `\0` 分隔, 所以 xargs 做了切分, 最后扔给 grep 过滤自己关心的变量.

#### 4. find container

```
for c in $(docker ps -q); do docker exec -it $c ip a | grep if16 -q && echo $c; done
```

这是查找包含 `if16` veth 设备的容器写的一个循环.

你们会发现, 我 99.9% 的 bash 使用场景都是单行的, 一次性的, 用完扔的, 直接在终端行 buffer 里修改和调试的代码, 十分高效.

所以我正式澄清我使用 bash 的三大场景:

1. 服务器上的一次性代码, 并且大部分都是无副作用的操作, 如查找;
2. 小型工程, 比如 `CI/CD` 的脚本, jenkins job 里运行的部署脚本;
3. 本地原型验证, 比如我的第一个 CNI 插件, 简单的 CNM 服务器, 简易实现 ssh;

那种大型的工程, 功能复杂到超过一个的事情, 都需要用 `getopt(1)` 去解析 argv 的, 我是绝对不会用 bash 去做的. Python click 去写正儿八经的项目不好吗, 多装几个 libs 怎么了, 反正是正经安装正经部署, 大不了发布镜像.

### 5.2 如果我一定要写工程

但是如果我一定要写工程呢, 那种反复运行, 还有大概率会新增功能, 我除了自杀还能怎么办?

#### 1. 检查变量

```
set -u
:$1 :$MY_ENV
```

`-u` 会对空字符串报错, 所以强硬一点, 对必须的环境变量和 arguments 率先做检查, 把空变量掐死在摇篮里.

#### 2. 中断异常

```
set -e
set -o pipefail
```

有了这个 flag, bash script 顺次运行时, 如果某命令退出码非零, 整个进程会立刻退出.

`pipefail` 能够在 pipeline 里中间命令异常退出时使得整个 pipeline 退出码非零.

#### 3. 做好 cleanup

```
function finish { rm -f /tmp/xxx; }
trap finish EXIT
```

trap 就是 `signal(2)` 的等价命令..

#### 4. hook

使用 `command_not_found_handle` 来注册找不到命令时候的回调吧, 挺好用的...

```
function command_not_found_handle() {
    echo "recv args: $1, $2"
}
wrong_command arg1 arg2
```

`arg1`, `arg2` 甚至会传给 `command_not_found_handle` 函数作为参数.

总体来说也没有特别的好办法.

---

最后希望大家愉快地利用 bash 提高生产力...逃
